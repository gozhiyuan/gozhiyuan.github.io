---
layout: post
title: Transformers and Autoregressive Models
subtitle: Deep Learning System 17
categories: CMU-Deep-Learning-Systems-2022
tags: [dlsys, transformer]
banner: "/assets/images/banners/yuanpang-wa-valley.jpg"
---


## ğŸ“š Transformers and Autoregressive Models

[Course Link](https://dlsyscourse.org/lectures/)

This document reviews the main themes and key takeaways from Deep Learning Systems: Algorithms and Implementation** at **Carnegie Mellon University**, taught by **J. Zico Kolter** and **Tianqi Chen**.

---


This document summarizes key concepts from the lecture on Transformers and attention mechanisms. The focus is on understanding how Transformers, initially developed for time series modeling, have become a dominant architecture in various deep learning applications. We explore core concepts, motivations, advantages, limitations, and applications beyond time series.

## 1. â³ Time Series Modeling: Two Approaches

### ğŸ”„ Recurrent Neural Network (RNN) - Latent State Approach
- **Concept:**  
  RNNs maintain a "latent state" that summarizes past information up to a given time point.  
  ğŸ§© _"The latent state (ht) acts as memory, accumulating information over time."_  
- **Pros:**  
  - ğŸ“œ _Potentially infinite history:_ Can capture long past dependencies.  
  - ğŸ—œï¸ _Compact representation:_ Entire history condensed into a single state.  
- **Cons:**  
  - ğŸ§® _Long compute path:_ Information from the distant past may vanish or explode through hidden states.  
  - âŒ _Difficult to incorporate long-term dependencies in practice._  

### ğŸ¯ Direct Prediction Approach
- **Concept:**  
  Directly maps input sequences to outputs without relying on latent states.  
  ğŸ§® _"Predict each Yt as a function of Xt without embedding state."_  
- **Pros:**  
  - âš¡ _Shorter compute paths:_ Efficient information capture.  
- **Cons:**  
  - â›” _No compact state representation:_ Entire history is processed for each prediction.  
  - ğŸ“ _Finite history:_ Limited by input size.  

---

## 2. ğŸ› ï¸ CNNs for Direct Prediction
- **Concept:**  
  Temporal Convolutional Networks (TCNs) use causal convolutions to ensure outputs depend only on past and current inputs.  
- **Causal Convolutions:**  
  â° _"Hidden states at time t depend only on states up to time t."_  
- **Limitations:**  
  - ğŸ” _Limited receptive field:_ Small receptive field, requiring deeper networks.  
- **Solutions:**  
  - ğŸ“ˆ _Dilated convolutions_  
  - ğŸŠ _Pooling layers_  
  Each solution has trade-offs like parameter increase or sparse inputs.  

---

## 3. ğŸ¯ Attention Mechanisms
- **Concept:**  
  Attention weights and combines states, computing a weighted sum over time.  
  ğŸ§‘â€ğŸ« _"Initially used in RNNs to combine latent states over all time points."_  

---

## 4. ğŸŒ Self-Attention
- **Concept:**  
  Attention where weights are determined by inputs (using queries, keys, and values).  
  ğŸ—ï¸ _"Self-attention uses Q (queries), K (keys), and V (values) matrices."_  
- **Operation:**  
  ```SelfAttention(Q, K, V) = softmax(QK^T / sqrt(d))V```  
- **Properties:**  
  - ğŸ”„ _Permutation Equivariance:_ Order of inputs doesn't affect result.  
  - ğŸŒ _Global Influence:_ Considers all time steps.  
  - ğŸ“Š _Constant parameter count:_ Entire sequence processed without increasing parameters.  
- **Compute Cost:**  
  - ğŸ’¸ _O(TÂ²d):_ Difficult to reduce.  

![alt_text](/assets/images/dlsys/17/1.png "image_tooltip")

---

## 5. ğŸš€ Transformer Architecture
- **Concept:**  
  Uses self-attention and feedforward layers to process sequences.  
  ğŸ”§ _"Transforms inputs to hidden states through a series of blocks."_  
- **Transformer Block:**  
  - ğŸ” Self-attention  
  - â• Residual connections  
  - âš–ï¸ Layer normalization  
  - ğŸ”¨ Feedforward network  
- **Parallel Processing:**  
  ğŸï¸ _Processes all time steps in parallel (unlike RNNs)._  
- **Advantages:**  
  - ğŸŒ _Full receptive field in a single layer._  
  - ğŸ› ï¸ _Mixes entire sequence without increasing parameters._  
- **Disadvantages:**  
  - â±ï¸ _Autoregressive tasks affected by dependencies on future inputs._  
  - ğŸ”„ _Permutation equivariance:_ No inherent data order capture.  

![alt_text](/assets/images/dlsys/17/2.png "image_tooltip")

---

## 6. ğŸ›¡ï¸ Addressing Limitations
- **Masked Self-Attention:**  
  ğŸ”’ _"Zero weight assigned to future steps to enforce causality."_  
- **Positional Encodings:**  
  ğŸ“Š _"Sinusoidal encodings added to capture sequence order."_  

---

## 7. ğŸ“ˆ Transformers Beyond Time Series
- **Vision Transformers (ViTs):**  
  ğŸ–¼ï¸ _Images represented as patch embeddings._  
- **Graph Transformers:**  
  ğŸ•¸ï¸ _Captures graph structures using modified attention._  
- **Challenges:**  
  - ğŸ§® Efficient computation of attention matrices  
  - ğŸ“ Effective positional embeddings  
  - ğŸ§± Mask matrix design  
