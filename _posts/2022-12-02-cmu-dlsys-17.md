---
layout: post
title: Transformers and Autoregressive Models
subtitle: Deep Learning System 17
categories: CMU-Deep-Learning-Systems-2022
tags: [dlsys, transformer]
banner: "/assets/images/banners/yuanpang-wa-valley.jpg"
---


## 📚 Transformers and Autoregressive Models

[Course Link](https://dlsyscourse.org/lectures/)

This document reviews the main themes and key takeaways from Deep Learning Systems: Algorithms and Implementation** at **Carnegie Mellon University**, taught by **J. Zico Kolter** and **Tianqi Chen**.

---


This document summarizes key concepts from the lecture on Transformers and attention mechanisms. The focus is on understanding how Transformers, initially developed for time series modeling, have become a dominant architecture in various deep learning applications. We explore core concepts, motivations, advantages, limitations, and applications beyond time series.

## 1. ⏳ Time Series Modeling: Two Approaches

### 🔄 Recurrent Neural Network (RNN) - Latent State Approach
- **Concept:**  
  RNNs maintain a "latent state" that summarizes past information up to a given time point.  
  🧩 _"The latent state (ht) acts as memory, accumulating information over time."_  
- **Pros:**  
  - 📜 _Potentially infinite history:_ Can capture long past dependencies.  
  - 🗜️ _Compact representation:_ Entire history condensed into a single state.  
- **Cons:**  
  - 🧮 _Long compute path:_ Information from the distant past may vanish or explode through hidden states.  
  - ❌ _Difficult to incorporate long-term dependencies in practice._  

### 🎯 Direct Prediction Approach
- **Concept:**  
  Directly maps input sequences to outputs without relying on latent states.  
  🧮 _"Predict each Yt as a function of Xt without embedding state."_  
- **Pros:**  
  - ⚡ _Shorter compute paths:_ Efficient information capture.  
- **Cons:**  
  - ⛔ _No compact state representation:_ Entire history is processed for each prediction.  
  - 📏 _Finite history:_ Limited by input size.  

---

## 2. 🛠️ CNNs for Direct Prediction
- **Concept:**  
  Temporal Convolutional Networks (TCNs) use causal convolutions to ensure outputs depend only on past and current inputs.  
- **Causal Convolutions:**  
  ⏰ _"Hidden states at time t depend only on states up to time t."_  
- **Limitations:**  
  - 🔍 _Limited receptive field:_ Small receptive field, requiring deeper networks.  
- **Solutions:**  
  - 📈 _Dilated convolutions_  
  - 🏊 _Pooling layers_  
  Each solution has trade-offs like parameter increase or sparse inputs.  

---

## 3. 🎯 Attention Mechanisms
- **Concept:**  
  Attention weights and combines states, computing a weighted sum over time.  
  🧑‍🏫 _"Initially used in RNNs to combine latent states over all time points."_  

---

## 4. 🌐 Self-Attention
- **Concept:**  
  Attention where weights are determined by inputs (using queries, keys, and values).  
  🗝️ _"Self-attention uses Q (queries), K (keys), and V (values) matrices."_  
- **Operation:**  
  ```SelfAttention(Q, K, V) = softmax(QK^T / sqrt(d))V```  
- **Properties:**  
  - 🔄 _Permutation Equivariance:_ Order of inputs doesn't affect result.  
  - 🌍 _Global Influence:_ Considers all time steps.  
  - 📊 _Constant parameter count:_ Entire sequence processed without increasing parameters.  
- **Compute Cost:**  
  - 💸 _O(T²d):_ Difficult to reduce.  

![alt_text](/assets/images/dlsys/17/1.png "image_tooltip")

---

## 5. 🚀 Transformer Architecture
- **Concept:**  
  Uses self-attention and feedforward layers to process sequences.  
  🔧 _"Transforms inputs to hidden states through a series of blocks."_  
- **Transformer Block:**  
  - 🔁 Self-attention  
  - ➕ Residual connections  
  - ⚖️ Layer normalization  
  - 🔨 Feedforward network  
- **Parallel Processing:**  
  🏎️ _Processes all time steps in parallel (unlike RNNs)._  
- **Advantages:**  
  - 🌐 _Full receptive field in a single layer._  
  - 🛠️ _Mixes entire sequence without increasing parameters._  
- **Disadvantages:**  
  - ⏱️ _Autoregressive tasks affected by dependencies on future inputs._  
  - 🔄 _Permutation equivariance:_ No inherent data order capture.  

![alt_text](/assets/images/dlsys/17/2.png "image_tooltip")

---

## 6. 🛡️ Addressing Limitations
- **Masked Self-Attention:**  
  🔒 _"Zero weight assigned to future steps to enforce causality."_  
- **Positional Encodings:**  
  📊 _"Sinusoidal encodings added to capture sequence order."_  

---

## 7. 📈 Transformers Beyond Time Series
- **Vision Transformers (ViTs):**  
  🖼️ _Images represented as patch embeddings._  
- **Graph Transformers:**  
  🕸️ _Captures graph structures using modified attention._  
- **Challenges:**  
  - 🧮 Efficient computation of attention matrices  
  - 📏 Effective positional embeddings  
  - 🧱 Mask matrix design  
