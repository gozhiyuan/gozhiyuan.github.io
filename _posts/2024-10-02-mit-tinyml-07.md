---
layout: post
title: TinyML Lecture 7 ğŸš€
subtitle: Neural Architecture Search
categories: Course-TLDR
tags: [tinyml, llm, pruning]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai ğŸ“š Lecture 7-8: Neural Architecture Search

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## Primitive Operations ğŸ”§
- **Fully Connected Layers**:  
  Connect every neuron in the input to every neuron in the output, characterized by weight matrices and bias vectors.  
  Example:  
  - Input: \( X \) with shape \( (n, c_i) \) (batch size \( n \), input channels \( c_i \))  
  - Output: \( Y \) with shape \( (n, c_o) \) (output channels \( c_o \))  
  - Weight: \( W \) with shape \( (c_o, c_i) \), Bias: \( b \) with shape \( (c_o) \).

- **Convolutional Layers**:  
  Extract features using kernels (e.g., 2D convolutions with input \( (C_{in}, H, W) \), weight \( (K_H, K_W, C_{in}, C_{out}) \)).

- **Group Convolutions**:  
  Divide input channels into groups for independent convolutions.

- **Depthwise Convolutions**:  
  Special case of group convolution with each input channel as a group.

- **1x1 Convolutions**:  
  Perform channel projection without spatial info.

---

## Building Blocks ğŸ—ï¸
- **ResNet Bottleneck Block**:  
  Combines \( 1 \times 1 \) (reduce channels), \( 3 \times 3 \) (extract features), and \( 1 \times 1 \) (restore channels).

- **ResNeXt Block**:  
  Extends ResNet with group convolutions for parallel paths.

- **MobileNet Depthwise Separable Block**:  
  Splits spatial (depthwise) and channel (pointwise) info processing.

- **MobileNet Inverted Bottleneck Block**:  
  Expands channels before depthwise convolution for better expressiveness.

- **Multi-Head Attention Block**:  
  Key for transformers; computes attention scores for sequence processing.

---

## Neural Architecture Search (NAS) ğŸ¤–
- **Motivation**:  
  Trade-offs between latency, accuracy, and resource efficiency. Manual design is time-consuming and suboptimal for specific hardware constraints.

- **Components**:
  - **Search Space**: Candidate architectures.
  - **Search Strategy**: Methods include:
    - *Grid Search*: Exhaustive but expensive.
    - *Random Search*: Simpler but less directed.
    - *Reinforcement Learning*: Uses rewards to guide exploration.
    - *Gradient Descent*: Optimizes differentiable architectures.
    - *Evolutionary Search*: Mutates and selects best-performing architectures.
  - **Performance Estimation**: Predicts architecture performance efficiently.

---

## Search Spaces ğŸ§©
- **Cell-Level Search**: Focuses on repeating a single cell design.
- **Network-Level Search**: Designs the entire architecture for flexibility.

---

## Hardware-Aware NAS âš™ï¸
- **Challenges**:  
  Searching directly on target tasks/hardware is expensive; proxies (like FLOPs) may be misleading.

- **Frameworks**:
  - **ProxylessNAS**:  
    - Builds an over-parameterized network.
    - Learns probabilities for paths and selects the best.
  - **Once-for-All (OFA)**:  
    - Trains a super-network to derive sub-networks for various hardware.

---

## ğŸ§ª NAS Applications Overview

### ğŸŒŸ Once-for-All (OFA) in Different Domains

#### ğŸ“– Natural Language Processing (NLP)
- **ğŸŒ€ Once-for-All Transformers (OFA-Transformers)**:  
  Design a single transformer model that can specialize into sub-networks for various platforms, from resource-constrained devices to powerful servers.  
- **âš™ï¸ Hardware-Aware Transformers (HAT)**:  
  Search for transformer architectures optimized for specific hardware platforms, achieving significant speed-ups and model size reductions.  
- **ğŸ”© Spartan Transformer Chip**:  
  A specialized chip for efficient NLP, showcasing the practical feasibility of hardware-aware NAS.

---

#### ğŸ”ï¸ 3D Vision
- **ğŸ›» Sparse Point Voxel (SPV) for LiDAR Segmentation**:  
  Efficient architectures designed to process point cloud data from LiDAR sensors, enabling real-time performance for self-driving applications.

---

#### ğŸ–¼ï¸ Image Editing (Generative Adversarial Networks - GANs)
- **ğŸ¨ Once-for-All GANs**:  
  Train a single GAN capable of generating images at various resolutions and quality levels, supporting fast prototyping and high-quality image finalization.

---

#### ğŸ•º Human Pose Estimation
- **ğŸ“± Hardware-aware NAS for Pose Estimation**:  
  Design efficient architectures for real-time pose estimation on mobile devices.

---

#### ğŸ§‘â€ğŸ”¬ Quantum AI
- **ğŸ”¬ OFA for Quantum Circuits**:  
  Search for robust sub-circuits within a larger super-circuit to mitigate noise and enhance accuracy in quantum computing applications.

---

#### ğŸ’¬ Large Language Models (LLMs)
- **âš¡ Inference Adaptive LLMs**:  
  Apply OFA principles to LLMs, creating multiple models of varying sizes from a single trained super-network, enabling deployment across diverse hardware platforms.
