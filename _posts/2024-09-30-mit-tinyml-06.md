---
layout: post
title: TinyML Lecture 6 ğŸš€
subtitle: Quantization II
categories: Course-TLDR
tags: [tinyml, llm, pruning]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai ğŸ“š Lecture 6: Quantization II

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## 1ï¸âƒ£ Recap of Quantization Concepts
- **ğŸ“Š K-means and Linear Quantization**:  
  Brief summary of K-means-based quantization (codebook) and linear quantization (scaling factors + zero points).  
- **âš–ï¸ Benefits and Trade-offs**:  
  - *Pros*: Reduced storage and computation costs.  
  - *Cons*: Potential accuracy degradation.

---

## 2ï¸âƒ£ Quantization Granularity
- **ğŸ“¦ Per-Tensor Quantization**:  
  - Single scaling factor for the entire tensor.  
  - Pros: Simplicity for large models.  
  - Cons: Accuracy issues in small models due to varying ranges across channels.  

- **ğŸ§± Per-Channel Quantization**:  
  - Individual scaling factors for each channel.  
  - Pros: Better accuracy for small models.  
  - Cons: Higher storage requirements for scaling factors.  

- **ğŸ‘¥ Group Quantization**:  
  - Reduces group size for finer scaling and improved accuracy at low precision.  
  - Importance in architectures like Blackw for low-bit quantization.

- **ğŸ“ Per-Vector Quantization (VSQuant)**:  
  - Combines a global floating-point scaling factor with integer per-vector scaling.  
  - Balances accuracy and hardware efficiency.

- **âš™ï¸ Shared Micro-exponent (MX) Data Type**:  
  - Combines shared exponent bits with per-channel/group scaling factors.  
  - Examples: MX4, MX6, MX9 with varying effective bit widths.

---

## 3ï¸âƒ£ Dynamic Range Clipping
- **âœ‚ï¸ Motivation for Clipping**:  
  Clipping minimizes quantization noise, especially for distributions with outliers.  

- **ğŸ“ Methods for Clipping**:  
  Techniques include exponential moving average, KL divergence minimization, and MSE minimization.  

- **ğŸ” Octave Technique**:  
  - Automatically finds optimal clipping ranges.  
  - Effectively maintains accuracy compared to FP32.

---

## 4ï¸âƒ£ Rounding in Quantization
- **ğŸ”„ Adaptive Rounding**:  
  - Challenges traditional "round-to-nearest" methods.  
  - Adaptive rounding considers correlated weights.  

- **ğŸ§  AdaRound Algorithm**:  
  - Learns optimal rounding decisions for weights.  
  - Minimizes reconstruction error while considering weight correlations.

---

## 5ï¸âƒ£ Quantization-Aware Training (QAT)
- **ğŸ¯ Concept and Benefits**:  
  - Models trained with quantization effects included.  
  - Improves accuracy compared to post-training quantization.  

- **ğŸ”§ Implementation**:  
  - Maintains full-precision weights during training.  
  - Quantized weights used during inference.  

- **â†”ï¸ Straight-Through Estimator (STE)**:  
  - Approximates gradients for non-differentiable quantization operations.

---

## 6ï¸âƒ£ Binary and Ternary Quantization
- **âš™ï¸ Motivation for Low-Precision Quantization**:  
  - Extreme savings in storage and computation using binary (1-bit) and ternary (2-bit) representations.  

- **ğŸ”¢ Binarization Techniques**:  
  - Deterministic (sign function) and stochastic binarization.  

- **ğŸ“‰ Accuracy Impact**:  
  - Scaling factors mitigate accuracy loss in binarized models.  

- **ğŸ¤– Binarized Neural Networks (BWN)**:  
  - Examples of BWN in tasks like image classification.  
  - Highlights trade-offs between accuracy and efficiency.  

- **â• Ternary Quantization**:  
  - Adds a zero value for more representational power.  
  - Threshold-based and trained ternary quantization (TTQ) with learnable scaling factors.

- **ğŸ”§ XNOR Operation and Popcount**:  
  - Efficient hardware implementation of binarized operations.

---

## 7ï¸âƒ£ Mixed-Precision Quantization
- **ğŸŒˆ Concept and Benefits**:  
  - Different layers/operations use varying bit widths.  
  - Optimizes the balance between accuracy and efficiency.  

- **ğŸ•µï¸ Design Space Exploration**:  
  - Challenges due to the vast design space.  
  - Automated solutions are crucial.  

- **ğŸ¤– Reinforcement Learning for Quantization**:  
  - Actor-critic frameworks optimize layer-specific bit widths.  

- **ğŸ”§ Hardware Considerations**:  
  - Specialized accelerators improve performance for mixed-precision models.  

- **ğŸ’¡ HAQ (Hardware-Aware Quantization)**:  
  - Example of a hardware-aware mixed-precision method.  
  - Outperforms uniform quantization.
