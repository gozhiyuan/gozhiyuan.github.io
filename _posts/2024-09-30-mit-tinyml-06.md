---
layout: post
title: TinyML Lecture 6 🚀
subtitle: Quantization II
categories: Course-TLDR
tags: [tinyml, llm, pruning]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai 📚 Lecture 6: Quantization II

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## 1️⃣ Recap of Quantization Concepts
- **📊 K-means and Linear Quantization**:  
  Brief summary of K-means-based quantization (codebook) and linear quantization (scaling factors + zero points).  
- **⚖️ Benefits and Trade-offs**:  
  - *Pros*: Reduced storage and computation costs.  
  - *Cons*: Potential accuracy degradation.

---

## 2️⃣ Quantization Granularity
- **📦 Per-Tensor Quantization**:  
  - Single scaling factor for the entire tensor.  
  - Pros: Simplicity for large models.  
  - Cons: Accuracy issues in small models due to varying ranges across channels.  

- **🧱 Per-Channel Quantization**:  
  - Individual scaling factors for each channel.  
  - Pros: Better accuracy for small models.  
  - Cons: Higher storage requirements for scaling factors.  

- **👥 Group Quantization**:  
  - Reduces group size for finer scaling and improved accuracy at low precision.  
  - Importance in architectures like Blackw for low-bit quantization.

- **📐 Per-Vector Quantization (VSQuant)**:  
  - Combines a global floating-point scaling factor with integer per-vector scaling.  
  - Balances accuracy and hardware efficiency.

- **⚙️ Shared Micro-exponent (MX) Data Type**:  
  - Combines shared exponent bits with per-channel/group scaling factors.  
  - Examples: MX4, MX6, MX9 with varying effective bit widths.

---

## 3️⃣ Dynamic Range Clipping
- **✂️ Motivation for Clipping**:  
  Clipping minimizes quantization noise, especially for distributions with outliers.  

- **📐 Methods for Clipping**:  
  Techniques include exponential moving average, KL divergence minimization, and MSE minimization.  

- **🔍 Octave Technique**:  
  - Automatically finds optimal clipping ranges.  
  - Effectively maintains accuracy compared to FP32.

---

## 4️⃣ Rounding in Quantization
- **🔄 Adaptive Rounding**:  
  - Challenges traditional "round-to-nearest" methods.  
  - Adaptive rounding considers correlated weights.  

- **🧠 AdaRound Algorithm**:  
  - Learns optimal rounding decisions for weights.  
  - Minimizes reconstruction error while considering weight correlations.

---

## 5️⃣ Quantization-Aware Training (QAT)
- **🎯 Concept and Benefits**:  
  - Models trained with quantization effects included.  
  - Improves accuracy compared to post-training quantization.  

- **🔧 Implementation**:  
  - Maintains full-precision weights during training.  
  - Quantized weights used during inference.  

- **↔️ Straight-Through Estimator (STE)**:  
  - Approximates gradients for non-differentiable quantization operations.

---

## 6️⃣ Binary and Ternary Quantization
- **⚙️ Motivation for Low-Precision Quantization**:  
  - Extreme savings in storage and computation using binary (1-bit) and ternary (2-bit) representations.  

- **🔢 Binarization Techniques**:  
  - Deterministic (sign function) and stochastic binarization.  

- **📉 Accuracy Impact**:  
  - Scaling factors mitigate accuracy loss in binarized models.  

- **🤖 Binarized Neural Networks (BWN)**:  
  - Examples of BWN in tasks like image classification.  
  - Highlights trade-offs between accuracy and efficiency.  

- **➕ Ternary Quantization**:  
  - Adds a zero value for more representational power.  
  - Threshold-based and trained ternary quantization (TTQ) with learnable scaling factors.

- **🔧 XNOR Operation and Popcount**:  
  - Efficient hardware implementation of binarized operations.

---

## 7️⃣ Mixed-Precision Quantization
- **🌈 Concept and Benefits**:  
  - Different layers/operations use varying bit widths.  
  - Optimizes the balance between accuracy and efficiency.  

- **🕵️ Design Space Exploration**:  
  - Challenges due to the vast design space.  
  - Automated solutions are crucial.  

- **🤖 Reinforcement Learning for Quantization**:  
  - Actor-critic frameworks optimize layer-specific bit widths.  

- **🔧 Hardware Considerations**:  
  - Specialized accelerators improve performance for mixed-precision models.  

- **💡 HAQ (Hardware-Aware Quantization)**:  
  - Example of a hardware-aware mixed-precision method.  
  - Outperforms uniform quantization.
