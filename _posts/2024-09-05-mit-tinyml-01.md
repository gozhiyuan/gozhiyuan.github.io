---
layout: post
title: TinyML Lecture 1 üöÄ
subtitle: Course Introduction
categories: Course TLDR
tags: [tinyml, llm]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---

# TinyML 2024 Lecture 1 üå±

MIT's *TinyML and Efficient Deep Learning Computing* course, taught by Professor Song Han, kicks off with an introduction to optimizing and speeding up deep learning models. As models grow in complexity, hardware constraints create a gap between model needs and deployment capabilities, driving up costs and emphasizing the need for efficient deep learning.

![alt_text](/assets/images/tinyml-2024/01/1.png "image_tooltip")

---

## üåü Key Highlights

### 1. The Need for Efficient Deep Learning üß†
- **Model Growth**: The exponential rise in model sizes, especially for LLMs, outpaces GPU memory.
- **Cost & Efficiency**: Larger models raise costs, reinforcing the demand for optimization.
- **Model Compression**: Techniques like pruning and quantization reduce model size and operational cost.

### 2. Computer Vision Applications üì∏
- **Applications**:
  - **Image Classification**: Super-human accuracy but high computation.
  - **Object Detection & Pose Estimation**: Optimized for mobile devices.
  - **Image Segmentation**: "Segment anything" models use EfficientViT-SAM to improve speed.
  - **Generative Models**: Diffusion models in image/video generation demand high computation.
  - **3D Vision**: Autonomous driving advancements with models like Fast-LiDARNet and BEVFusion.
- **Efficiency Focus**: Balancing high accuracy with minimal computation.

#### EfficientViT-SAM: Accelerating "Segment Anything" ‚ö°
![alt_text](/assets/images/tinyml-2024/01/2.png "image_tooltip")
EfficientViT-SAM enhances SAM-ViT-H by integrating EfficientViT, boosting speed without losing accuracy.

- **What is "Segment Anything"?** Models that can segment any object in an image with a prompt (e.g., point, box).
- **Efficiency Gains**:
  - **48.9x Throughput**: Processes 49x more images/sec on A100 GPU.
  - **Zero-Shot Segmentation**: Matches SAM-ViT-H‚Äôs performance without training.
- **Advantages**:
  - **Reduced Computation**: Enables real-time use on limited hardware.
  - **Preserved Accuracy**: Same performance as SAM-ViT-H.

### 3. BEVFusion for 3D Perception üåê
![alt_text](/assets/images/tinyml-2024/01/3.png "image_tooltip")
BEVFusion enables efficient 3D perception in self-driving:
- **Sensor Input**: Combines data from six cameras and LiDAR.
- **Tasks**:
  - **3D Object Detection**: Identifies objects in 3D, like cars and pedestrians.
  - **BEV Map Segmentation**: Creates a top-down map, segmenting areas like lanes and barriers.
- **Efficiency**: Runs on NVIDIA Jetson Orin, a mobile GPU, for real-time use.

### 4. Natural Language Processing & Large Language Models üìù
- **LLM Capabilities**: Tasks include translation, code generation, few-shot learning.
- **Challenges**: Bigger models demand more GPU memory, limiting edge deployment.
- **Efficient Techniques**: Token pruning (SpAtten) and quantization (SmoothQuant, AWQ).
- **Edge Deployment**: Privacy and offline needs drive edge deployment efforts.

### 5. Multimodal Learning and Vision-Language Models üé•üìù
- **Vision-Language Models**: Covers models like LLaVA for image-text tasks (e.g., captioning).
- **Efficiency**: Uses quantization methods (AWQ) for computational efficiency.
- **Data and Training**: High-quality data and robust training essential for multimodal learning.

### 6. Hardware Trends and the Role of Software üíæ
- **GPU Trends**: Higher performance and memory bandwidth with increased power usage.
- **Mobile & Edge Hardware**: Specialized AI units (e.g., Qualcomm Hexagon) but limited memory.
- **Microcontrollers**: TinyML needs extreme efficiency due to tight memory constraints.
- **Software Optimization**: Critical for maximizing hardware potential in complex models.

#### Debriefing AI Hardware: Cloud vs. Edge üñ•Ô∏èüì±

Cloud GPUs and edge devices highlight different strengths:
- **Cloud GPUs**: Performance gains from P100 to B100 (2024) offer 100x dense FP16 performance; memory bandwidth doubled.
- **Edge Hardware**: Mobile AI units like Qualcomm's Hexagon DSP allow efficient on-device AI, while Jetson targets high-performance applications.
- **Memory Gap**: Cloud GPUs support GBs of memory, MCUs only a few KBs, necessitating model compression for edge use.

### 7. System-Algorithm Co-Design ü§ù
- **System-Algorithm Interaction**: Understanding hardware-algorithm interplay is crucial.
- **Full-Stack Optimization**: Combining EE, CS, and AI knowledge to drive innovation.

### 8. Course Logistics üìÖ
- **Prerequisites**: Computer architecture (6.191) and machine learning (6.390).
- **Hands-On Labs**: Includes pruning, quantization, neural architecture search, LLM compression, and deployment.
- **Grading**: Labs (75%), final project (25%), 4% participation bonus.

---

## üìù Key Takeaways
- **Efficiency as a Priority**: Rising model complexity demands efficient solutions.
- **Model Compression**: Techniques like pruning and quantization are essential.
- **System-Algorithm Co-Design**: Holistic approach for high performance and efficiency.
- **Comprehensive Learning**: Prepares students to tackle modern AI challenges.

This lecture lays the foundation for exploring efficient deep learning throughout the semester.

## Quiz üìù

### Instructions
Answer the following questions in **2-3 sentences** each.

---

### Questions üîç

1. **Why is efficient deep learning computing increasingly necessary?**  
2. **What event in 2012 marked a significant shift in the field of deep learning, and what was its primary contribution?**  
3. **What are two advantages of on-device training for AI systems?**  
4. **Describe one technique used to improve the efficiency of promptable image segmentation models.**  
5. **Why are diffusion models computationally expensive, especially for generating high-resolution images or videos?**  
6. **What is the primary challenge in aligning representations from different input modalities in multimodal learning?**  
7. **How does the concept of "chain of thought" enhance the problem-solving capabilities of large language models?**  
8. **Explain the rationale behind using pruning techniques to accelerate language models.**  
9. **What is the main advantage of using quantization techniques like SmoothQuant and AWQ for deploying large language models on edge devices?**  
10. **Explain the concept of visual in-context learning in the context of vision-language models.**  

---

### Answer Key üóùÔ∏è

1. **Efficient Deep Learning Computing Necessity**  
   Efficient deep learning computing is crucial because the computational demands of AI models are increasing rapidly, outpacing hardware growth. This disparity creates challenges in cost, energy consumption, and deploying models on resource-constrained devices. ‚öôÔ∏è

2. **2012 Breakthrough in Deep Learning**  
   The advent of *AlexNet* in 2012 revolutionized deep learning by showcasing the effectiveness of CNNs for image classification on ImageNet. AlexNet's deep architecture and GPU usage sparked significant advancements in computer vision and deep learning. üìà

3. **On-Device Training Advantages**  
   On-device training enhances *privacy* by processing data locally, removing the need to send sensitive data to the cloud. It also reduces *costs* by minimizing data transfer and cloud computing expenses, making AI applications more accessible. üîíüí∏

4. **Efficient Promptable Image Segmentation**  
   EfficientViT is an optimized Vision Transformer (ViT) architecture used for promptable image segmentation. It speeds up segmentation models like SAM by minimizing redundancy and streamlining the processing pipeline. üöÄ

5. **Computational Cost of Diffusion Models**  
   Diffusion models are computationally intensive due to their iterative denoising process. Generating high-resolution images or videos requires numerous steps, each needing substantial computation. üíª

6. **Challenge in Multimodal Learning**  
   The primary challenge in multimodal learning is aligning different input modalities (images, text, audio) into a shared representation space. This alignment is essential for the model to understand relationships between modalities and make accurate predictions. üé®üìù

7. **"Chain of Thought" in Large Language Models**  
   "Chain of thought" prompting guides LLMs to break down problems into smaller reasoning steps, enabling more logical solutions. This approach improves the model's ability to solve complex tasks. üîóüß†

8. **Pruning for Language Model Acceleration**  
   Pruning reduces model size by removing redundant connections in neural networks, or tokens in language models, to improve efficiency. This approach speeds up inference without significantly impacting performance. ‚úÇÔ∏è

9. **Quantization for Edge Device Deployment**  
   Techniques like SmoothQuant reduce numerical precision of model parameters, typically from 16-bit to lower bit-widths like 4-bit. This allows large models to fit within edge device memory while maintaining accuracy. üî¢üì±

10. **Visual In-Context Learning in Vision-Language Models**  
    Visual in-context learning enables models to understand tasks from a few labeled examples, without explicit instructions. For example, a model can learn to identify cities in images after being shown a few image-city pairs. üñºÔ∏èüîç

---

### Essay Questions üñãÔ∏è

1. **Trade-Offs Between Model Accuracy and Efficiency**  
   Discuss the trade-offs in deploying deep learning models on edge devices, considering computational resources, power constraints, and application requirements.

2. **Neural Architecture Search (NAS) in Efficient Model Development**  
   Explain the role of NAS in designing efficient deep learning models. Compare different NAS approaches, their strengths, and limitations. 

3. **Challenges of Training Large Language Models on Resource-Constrained Devices**  
   Describe techniques to reduce the computational and memory requirements of training LLMs on resource-constrained devices. Discuss associated challenges and opportunities.

4. **Ethical Considerations in Deploying Efficient Deep Learning Models**  
   Consider the ethics of deploying efficient AI models in areas like facial recognition and autonomous driving, including potential biases and privacy concerns.

5. **Future Directions in Efficient Deep Learning Computing**  
   Explore future directions in efficient deep learning, considering new hardware architectures, novel compression techniques, and AI sustainability.

---

### Glossary üìò

- **AlexNet**: A groundbreaking CNN architecture that highlighted deep learning's power for image classification.
- **Chain of Thought Prompting**: Enhances LLM reasoning by guiding them to break down complex problems into steps.
- **Diffusion Model**: Generative model creating images/videos via iterative denoising from random noise.
- **Efficient Deep Learning Computing**: Field dedicated to reducing computational and memory demands of deep learning, enabling deployment on resource-limited devices.
- **GAN Compression**: Reduces computational cost in GANs through pruning, knowledge distillation, etc.
- **ImageNet**: Large dataset of labeled images for training/evaluating computer vision models.
- **Large Language Model (LLM)**: Model trained on massive text data, generating human-like text and language tasks.
- **Multimodal Learning**: Processes and understands information from multiple modalities (images, text, etc.).
- **Neural Architecture Search (NAS)**: Automates neural network design, optimizing configurations.
- **On-Device Training**: Training AI models directly on edge devices, benefiting privacy, cost, and customization.
- **Pruning**: Compresses networks by removing connections/neurons deemed less important.
- **Quantization**: Reduces precision of numerical values in model parameters, lowering model size.
- **SmoothQuant**: Quantization technique that smooths activation distribution, improving quantization.
- **TinyML**: Focuses on deploying deep learning models on ultra-low-power microcontrollers.
- **Vision Transformer (ViT)**: Applies transformer architecture (for language) to vision tasks.
- **Visual In-Context Learning**: Vision-language model capability to infer tasks from few visual examples.
- **Visual Language Model**: Trained on image-text data, understanding and generating visual and textual info.

---  


