---
layout: post
title: Deep Learning System 5
subtitle: automatic differentiation introduction lab
categories: Course-TLDR dlsys
tags: [dlsys]
banner: "/assets/images/banners/yuanpang-wa-valley.jpg"
---


## ğŸ“š Deep Learning Systems Lecture 5

[Course Link](https://dlsyscourse.org/lectures/)

This document reviews the main themes and key takeaways from Deep Learning Systems: Algorithms and Implementation** at **Carnegie Mellon University**, taught by **J. Zico Kolter** and **Tianqi Chen**.

---

## ğŸ” The "Needle" Package: A Deep Dive into Automatic Differentiation Implementation

The **"Needle"** package is a compact yet powerful framework for implementing automatic differentiation (AD). **"Needle"** stands for **Necessary Elements of Deep Learning**, highlighting its focus on core components required to build a deep learning library from scratch. ğŸš€

[Source Code Link](https://github.com/dlsyscourse/lecture5/blob/main/5_automatic_differentiation_implementation.ipynb)

---

## ğŸ“‚ Structure and Content

The "Needle" package consists of three primary files:

- **`__init__.py`**: Handles all necessary imports for the library.
- **`autograd.py`**: Defines essential data structures and the mechanisms for AD. (~400 lines of code).
- **`ops.py`**: Contains operator definitions crucial for various operations (~300 lines of code).

In total, **"Needle"** is a lightweight framework with around **1,000 lines of code**, making it compact yet powerful.

---

## ğŸ”‘ Key Data Structures and Classes

### 1. **`Value` Class**
- Represents a node in the computational graph.
- Stores:
  - **`cached_data`**: Numerical data.
  - **`inputs`**: List of input nodes.
  - **`op`**: Operation used to compute the value.
  - **`requires_grad`**: Flag indicating if gradients are needed.

### 2. **`Tensor` Class**
- Subclass of `Value`.
- Acts as the **user-facing interface** for interacting with tensors.


### 3. **`Op` Class**
- Defines operations within the computational graph.
- Each subclass of `Op` implements two critical functions:
  - **`compute`**: Calculates the output data based on input data using an array API (initially NumPy).
  - **`gradient`**: Computes input adjoints given the output adjoint, enabling reverse mode AD.

---

## ğŸ§µ **Tensor.make_from_op in the Needle Framework**

The `Tensor.make_from_op` method is a static method within the `Tensor` class, playing a critical role in constructing the computational graph in Needle. It creates new `Tensor` objects that represent nodes in the graph and populates them with essential computation details.

### ğŸ—ï¸ **Internal Construction: Using `__new__`**
Instead of using the standard `__init__` constructor, `Tensor.make_from_op` utilizes the `__new__` method to create `Tensor` objects. 

- **Why `__new__`?**
  - The `__init__` method in the `Tensor` class is already overloaded to handle user-facing tensor creation from NumPy arrays and other inputs.
  - `__new__` provides an alternative path for internal construction, bypassing user-facing logic.


### ğŸ“ **Populating Attributes: The `_init` Function**
Once the `Tensor` object is created with `__new__`, `make_from_op` calls the internal `_init` function (from the base `Value` class) to populate the object's attributes:

- **`op`**: The operation (`Op` object) used to compute the tensor's value.
- **`inputs`**: A list of `Value` objects representing the inputs to the operation.
- **`requires_grad`**: A flag indicating whether the tensor requires gradient computation, determined by checking if any input requires gradients.

### âš™ï¸ **Data Initialization and Execution Modes**
The behavior of `make_from_op` varies based on the execution mode:

- **Eager Mode** ğŸƒâ€â™‚ï¸:
  - Immediately calls `realize_cached_data`, which computes the tensor's value based on its inputs and stores the result in `cached_data`.
  
- **Lazy Mode** ğŸ’¤:
  - Defers computation. `cached_data` remains unpopulated until explicitly required, such as when accessing the tensor's data or shape.

---

## ğŸ” **Example: Illustrating the Process**

Let's break down the creation of a tensor `z` from the operation `z = x + y`:

1. **Operator Overloading** âš¡:
   - The addition (`+`) triggers the `__add__` method of the `Tensor` class, which internally calls the `EwiseAdd` operator from `needle.ops`.

2. **Op Call and `make_from_op`** â¡ï¸:
   - `EwiseAdd`â€™s `__call__` method invokes `Tensor.make_from_op`.

3. **Node Creation and Attribute Initialization** ğŸ”§:
   - `make_from_op` creates a new `Tensor` object `z` and initializes its attributes:
     - **`op`**: `EwiseAdd` (the addition operation)
     - **`inputs`**: `[x, y]` (input tensors)
     - **`requires_grad`**: Based on whether `x` or `y` requires gradients.

4. **Data Initialization (Eager vs. Lazy)** ğŸ› ï¸:
   - **Eager Mode**:
     - Calls `realize_cached_data`, computes `z = x + y`, and stores the result in `z.cached_data`.
   - **Lazy Mode**:
     - Leaves `z.cached_data` as `None`, deferring computation until needed.


---

## âš™ï¸ Execution Modes

### 1. **Eager Mode** ğŸƒâ€â™‚ï¸
- Default mode, similar to PyTorch.
- Computations are performed **immediately** when operations are defined.
- **Pros**: Masks graph construction costs by interleaving graph construction and execution.

### 2. **Lazy Mode** ğŸ’¤
- Defers computations until explicitly required (e.g., printing or checking shape).
- **Pros**: Optimizations for larger computational graphs.

---

## ğŸ”„ Automatic Differentiation (AD) Implementation

### 1. **`gradient` Function**
- Defined within each `Op` subclass.
- **Engine** of reverse mode AD, calculating input adjoints from the output adjoint.

---

### 2. **Computational Graph Traversal**
- The **`gradient_as_tuple`** function streamlines traversal by ensuring the gradient function returns a tuple of adjoints.
- The reverse mode AD algorithm recursively traverses the computational graph, leveraging each `Op`'s `gradient` function to compute adjoints.

---

### 3. **Extended Computational Graph**
- Instead of directly computing adjoints, reverse mode AD in **"Needle"** constructs a **new computational graph** representing adjoint computations.
- **Benefits**:
  - Facilitates **gradient-of-gradient** operations.
  - Provides opportunities for further **optimizations**.

---

## ğŸ› ï¸ Addressing Potential Issues

### 1. **Memory Management** ğŸ§ 
- Naively accumulating values in loops can create extended chains within the computational graph, leading to memory issues.

### 2. **`detach` Function** ğŸ›‘
- Similar to PyTorch's `stop_gradient` or `no_autograd`.
- **Detaches** a tensor from the computational graph, creating a standalone tensor without history.
- **Prevents memory leaks** by stopping gradient propagation.

---

## ğŸŒŸ Future Development

The sources highlight that **"Needle"** has the potential to evolve into a full-fledged deep learning framework. Future developments may include:

- Replacing NumPy arrays with a **custom multi-dimensional array library**.
- Enabling **GPU acceleration**.
- Supporting more sophisticated deep learning models.

---

## ğŸ‰ Conclusion

**"Needle"** is a concise yet powerful package that encapsulates the essential elements of deep learning. Despite its small codebase, it demonstrates how to build a robust framework for automatic differentiation and computational graph construction, laying the groundwork for future expansion. ğŸ’¡
