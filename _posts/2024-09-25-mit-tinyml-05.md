---
layout: post
title: TinyML Lecture 5 🚀
subtitle: Quantization I
categories: Course-TLDR
tags: [tinyml, llm, pruning]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai 📚 Lecture 5: Quantization I

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## I. Introduction 🚀

This section introduces **quantization** as a method to reduce the size and computational cost of neural network models by lowering the precision of parameters. It outlines the lecture agenda:
- 📋 Reviewing numeric data types.
- 🤖 Basics of neural network quantization.
- 🔍 Exploring quantization approaches (K-means, linear, binary, and ternary).

---

## II. Numeric Data Types 🔢

### A. Integers ✏️
- **Unsigned vs. Signed** integers.
- Explains **sign-magnitude** representation and its limitations.
- Introduces **two's complement** for signed integers.

### B. Fixed-Point Numbers 📐
- Introduces **fixed-point numbers** with integer and fractional bits.
- Representation via **two's complement** and value interpretation.

### C. Floating-Point Numbers 🌊
- **IEEE 754 standard** for 32-bit floating-point numbers.
- Components: **sign bit, exponent bits, fraction bits**.
- Example of floating-point representation calculation.
- **Subnormal numbers** for representing zero.
- Special values: **positive/negative infinity and NaN**.

### D. Floating-Point Precision Variations 🔍
- **FP32, FP16, BF16** formats and trade-offs:
  - BF16: Larger dynamic range, less precision.
  
### E. FP8 and INT4/FP4 Representations 🎛️
- Nvidia's **FP8** format (E4M3, E5M2 configurations).
- **INT4/FP4** for weight and gradient representation.

---

## III. Introduction to Quantization 📉

Defines quantization: converting continuous values into a discrete set. Includes visual examples (signals/images) and highlights minimizing **quantization error**.

---

## IV. K-Means-Based Quantization 📊

### A. Weight Quantization Process 🧮
- Uses **K-means clustering** for weight quantization.
- Saves storage by storing **indices** and **codebooks**.

### B. Fine-Tuning Quantized Weights 🔧
- Group gradients by centroids, update centroids during training.

### C. Accuracy vs. Compression 🎯
- Example: AlexNet.
- Compares **quantization**, **pruning**, and **combined approaches**.

### D. Weight Distribution and Number of Bits 📉
- Discretization into **centroids**.
- Practical bit choices: **4 bits** (convolution), **2 bits** (fully connected).

### E. Huffman Coding 📦
- Compression using **non-uniform weight distributions**.

### F. Deep Compression Pipeline 🛠️
- Stages: **pruning → quantization → Huffman coding**.
- Demonstrates high compression with retained accuracy.

### G. Computation with K-means Quantization 🖥️
- Storage savings but no computational savings.
- Decoding weights requires floating-point operations.

---

## V. Linear Quantization ➕➗

### A. Introduction and Concept 🎯
- Integer weights for **storage and computation savings**.
- Affine mapping using a **scaling factor (S)** and **zero-point (Z)**.

### B. Linear Quantization Formula and Parameters 📝
- Mapping between floating-point and integer ranges.
- Visual demonstration of the formula.

### C. Calculation of Scaling Factor and Zero-Point ⚖️
- Derivation of scaling factor based on dynamic ranges.
- Formula for **zero-point**.

### D. Linear Quantized Matrix Multiplication 🔢
- Matrix multiplication with **integer arithmetic**.
- Rescaling for efficient bit-shift operations.

### E. Symmetric Linear Quantization 🔄
- Simplifies computations by setting **zero-point to zero**.

### F. Incorporating Bias in Linear Quantization ➕
- Quantization and computation of bias with integers.

### G. Linear Quantized Convolution Layer 🔄
- Applies linear quantization to convolutional layers.

### H. Computational Diagram and Accuracy Comparison 📊
- Diagram: Computational flow of quantized convolution.
- Efficiency comparison: **floating-point vs. integer-quantized models**.
