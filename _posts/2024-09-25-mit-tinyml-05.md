---
layout: post
title: TinyML Lecture 5 ğŸš€
subtitle: Quantization I
categories: Course-TLDR
tags: [tinyml, llm, pruning]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai ğŸ“š Lecture 5: Quantization I

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## I. Introduction ğŸš€

This section introduces **quantization** as a method to reduce the size and computational cost of neural network models by lowering the precision of parameters. It outlines the lecture agenda:
- ğŸ“‹ Reviewing numeric data types.
- ğŸ¤– Basics of neural network quantization.
- ğŸ” Exploring quantization approaches (K-means, linear, binary, and ternary).

---

## II. Numeric Data Types ğŸ”¢

### A. Integers âœï¸
- **Unsigned vs. Signed** integers.
- Explains **sign-magnitude** representation and its limitations.
- Introduces **two's complement** for signed integers.

### B. Fixed-Point Numbers ğŸ“
- Introduces **fixed-point numbers** with integer and fractional bits.
- Representation via **two's complement** and value interpretation.

### C. Floating-Point Numbers ğŸŒŠ
- **IEEE 754 standard** for 32-bit floating-point numbers.
- Components: **sign bit, exponent bits, fraction bits**.
- Example of floating-point representation calculation.
- **Subnormal numbers** for representing zero.
- Special values: **positive/negative infinity and NaN**.

### D. Floating-Point Precision Variations ğŸ”
- **FP32, FP16, BF16** formats and trade-offs:
  - BF16: Larger dynamic range, less precision.
  
### E. FP8 and INT4/FP4 Representations ğŸ›ï¸
- Nvidia's **FP8** format (E4M3, E5M2 configurations).
- **INT4/FP4** for weight and gradient representation.

---

## III. Introduction to Quantization ğŸ“‰

Defines quantization: converting continuous values into a discrete set. Includes visual examples (signals/images) and highlights minimizing **quantization error**.

---

## IV. K-Means-Based Quantization ğŸ“Š

### A. Weight Quantization Process ğŸ§®
- Uses **K-means clustering** for weight quantization.
- Saves storage by storing **indices** and **codebooks**.

### B. Fine-Tuning Quantized Weights ğŸ”§
- Group gradients by centroids, update centroids during training.

### C. Accuracy vs. Compression ğŸ¯
- Example: AlexNet.
- Compares **quantization**, **pruning**, and **combined approaches**.

### D. Weight Distribution and Number of Bits ğŸ“‰
- Discretization into **centroids**.
- Practical bit choices: **4 bits** (convolution), **2 bits** (fully connected).

### E. Huffman Coding ğŸ“¦
- Compression using **non-uniform weight distributions**.

### F. Deep Compression Pipeline ğŸ› ï¸
- Stages: **pruning â†’ quantization â†’ Huffman coding**.
- Demonstrates high compression with retained accuracy.

### G. Computation with K-means Quantization ğŸ–¥ï¸
- Storage savings but no computational savings.
- Decoding weights requires floating-point operations.

---

## V. Linear Quantization â•â—

### A. Introduction and Concept ğŸ¯
- Integer weights for **storage and computation savings**.
- Affine mapping using a **scaling factor (S)** and **zero-point (Z)**.

### B. Linear Quantization Formula and Parameters ğŸ“
- Mapping between floating-point and integer ranges.
- Visual demonstration of the formula.

### C. Calculation of Scaling Factor and Zero-Point âš–ï¸
- Derivation of scaling factor based on dynamic ranges.
- Formula for **zero-point**.

### D. Linear Quantized Matrix Multiplication ğŸ”¢
- Matrix multiplication with **integer arithmetic**.
- Rescaling for efficient bit-shift operations.

### E. Symmetric Linear Quantization ğŸ”„
- Simplifies computations by setting **zero-point to zero**.

### F. Incorporating Bias in Linear Quantization â•
- Quantization and computation of bias with integers.

### G. Linear Quantized Convolution Layer ğŸ”„
- Applies linear quantization to convolutional layers.

### H. Computational Diagram and Accuracy Comparison ğŸ“Š
- Diagram: Computational flow of quantized convolution.
- Efficiency comparison: **floating-point vs. integer-quantized models**.
