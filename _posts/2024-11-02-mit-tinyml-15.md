---
layout: post
title: TinyML Lecture 15 üöÄ
subtitle: Long-Context LLM 
categories: Course-TLDR
tags: [tinyml, llm, llm-deployment]
banner: "/assets/images/banners/yuanpang-wa-sky.jpg"
---


## EfficientML.ai üìö Lecture 15: Long-Context LLM 

Modern AI models are becoming increasingly large, demanding substantial computational resources and memory. This creates a gap between the computational demands of these models and the available hardware capabilities. Pruning addresses this gap by reducing model size, memory footprint, and ultimately, energy consumption.

[Course link](https://hanlab.mit.edu/courses/2024-fall-65940)


## üõ†Ô∏è I. Introduction

This briefing document summarizes key concepts and techniques presented in the **EfficientML.ai Lecture 15 (MIT 6.5940, Fall 2024)**, focusing on how to efficiently handle long-context large language models. The lecture addresses the challenges of processing extended text, video, and multimodal data, exploring methods to reduce memory consumption and improve processing speed for both training and inference. The primary issues tackled are:

![alt_text](/assets/images/tinyml-2024/15/1.png "image_tooltip")

- **Context Length Limitations**: Traditional LLMs have a limited context window (e.g., 2k tokens for LLaMA, 4k for Llama-2), which restricts their ability to process longer documents or conversations effectively.
- **Computational Cost**: The attention mechanism's quadratic complexity with respect to sequence length is a major bottleneck, especially for long contexts.
- **Memory Consumption**: The Key-Value (KV) cache grows linearly with sequence length, leading to excessive memory use, especially for large models and batch sizes.
- **Loss-in-the-Middle Phenomenon**: LLMs tend to perform worse on information in the middle of long contexts compared to the beginning or end.

---

## üß† II. Key Themes and Concepts

### üîó A. Context Extension

#### üîÑ Rotary Positional Embedding (RoPE):
- RoPE is a popular relative positional embedding technique that encodes positional information into token embeddings by rotating them in 2D space.
- Extends the context window (e.g., 4k for Llama-2) by adjusting the frequency parameter (Œ∏) to "double the frequency."
- Fine-tuning is required after extending the context length.

#### üêç LongLoRA:
- **Efficient Fine-tuning**: Uses shifted sparse attention to fine-tune models for long contexts on one GPU, supporting up to 32k tokens.
- **Shifted Sparse Attention**: Tokens only attend within groups, with cross-group interaction via shifted attention masks.
- **Enhanced LoRA**: Fine-tunes not just LoRA branches but also input embeddings and normalization layers for improved results.

---

### üß™ B. Evaluation of Long-Context LLMs

#### ‚ö†Ô∏è Lost-in-the-Middle Phenomenon:
- LLMs struggle to accurately recall information in the middle of long contexts, despite fluency.
- Perplexity is insufficient for evaluating long-context performance.

#### üìù Long-Context Benchmarks:
1. **Needle In A Haystack (NIAH)**: Tests memory retrieval for specific information at different positions.
2. **LongBench**: A dataset supporting real-world tasks up to 13k tokens, emphasizing both synthetic and practical evaluations.

---

### ‚ö° C. Efficient Attention Mechanisms

#### üíæ KV Cache Recap:
- The KV cache grows linearly with sequence length, becoming a memory bottleneck.

#### üåä StreamingLLM:
- Handles long conversations with a sliding window approach while preserving attention sinks.
- Adds a **learnable sink token** to improve efficiency.

#### ‚öîÔ∏è DuoAttention:
- Uses **Retrieval Heads** (for earlier tokens) and **Streaming Heads** (for recent tokens) to reduce memory usage and latency.
- Assigns trainable gates during training, binarized during inference.

#### üßπ Query-Aware Sparsity:
- Dynamically prunes the KV cache based on query importance.
- Preserves high accuracy while reducing memory movement and improving inference speed.

---

### üöÄ D. Beyond Transformers

#### üß¨ State-Space Models (SSMs): Mamba
- Offers an alternative to attention with constant KV cache size and linear time complexity.
- Improves sequence modeling using selective state-space mechanisms.

#### ‚ö° Hybrid Models: Jamba
- Combines Transformer and Mamba layers to leverage the strengths of both architectures.
- Supports up to **256k tokens** on an 80GB GPU.

---

## üîë III. Key Takeaways

- Extending LLM context length is vital for handling long documents, videos, and conversations.
- **RoPE** enables context window extension but requires fine-tuning.
- **LongLoRA** efficiently fine-tunes models for long contexts with minimal complexity.
- Benchmarks like **NIAH** and **LongBench** are critical for evaluating long-context performance.
- Mechanisms like **StreamingLLM**, **DuoAttention**, and **Query-Aware Sparsity** enhance memory and latency efficiency.
- Alternatives like **Mamba** and hybrids like **Jamba** provide innovative approaches for long-context processing.
