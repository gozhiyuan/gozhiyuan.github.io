<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-03T01:41:06-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">YuanPang Blog</title><subtitle>Welcome to my technical blog, a comprehensive resource for enthusiasts and professionals alike.  Dive into a wealth of learning notes and insights on  machine learning, deep learning, AI, large language models (LLMs), generative models,  data science, computer science, natural language processing (NLP), and computer vision.  Stay updated with the latest advancements, explore practical tutorials,  and enhance your knowledge in these cutting-edge fields.  Whether you&apos;re a beginner or an expert, this blog is your go-to destination for all things AI and beyond.</subtitle><author><name>Leon Liu</name></author><entry><title type="html">TinyML Lecture 1 üöÄ</title><link href="http://localhost:4000/course/tldr/2024/09/05/mit-tinyml-01.html" rel="alternate" type="text/html" title="TinyML Lecture 1 üöÄ" /><published>2024-09-05T00:00:00-04:00</published><updated>2024-09-05T00:00:00-04:00</updated><id>http://localhost:4000/course/tldr/2024/09/05/mit-tinyml-01</id><content type="html" xml:base="http://localhost:4000/course/tldr/2024/09/05/mit-tinyml-01.html"><![CDATA[<h1 id="tinyml-2024-lecture-1-">TinyML 2024 Lecture 1 üå±</h1>

<p>MIT‚Äôs <em>TinyML and Efficient Deep Learning Computing</em> course, taught by Professor Song Han, kicks off with an introduction to optimizing and speeding up deep learning models. As models grow in complexity, hardware constraints create a gap between model needs and deployment capabilities, driving up costs and emphasizing the need for efficient deep learning.</p>

<p><img src="/assets/images/tinyml-2024/01/1.png" alt="alt_text" title="image_tooltip" /></p>

<hr />

<h2 id="-key-highlights">üåü Key Highlights</h2>

<h3 id="1-the-need-for-efficient-deep-learning-">1. The Need for Efficient Deep Learning üß†</h3>
<ul>
  <li><strong>Model Growth</strong>: The exponential rise in model sizes, especially for LLMs, outpaces GPU memory.</li>
  <li><strong>Cost &amp; Efficiency</strong>: Larger models raise costs, reinforcing the demand for optimization.</li>
  <li><strong>Model Compression</strong>: Techniques like pruning and quantization reduce model size and operational cost.</li>
</ul>

<h3 id="2-computer-vision-applications-">2. Computer Vision Applications üì∏</h3>
<ul>
  <li><strong>Applications</strong>:
    <ul>
      <li><strong>Image Classification</strong>: Super-human accuracy but high computation.</li>
      <li><strong>Object Detection &amp; Pose Estimation</strong>: Optimized for mobile devices.</li>
      <li><strong>Image Segmentation</strong>: ‚ÄúSegment anything‚Äù models use EfficientViT-SAM to improve speed.</li>
      <li><strong>Generative Models</strong>: Diffusion models in image/video generation demand high computation.</li>
      <li><strong>3D Vision</strong>: Autonomous driving advancements with models like Fast-LiDARNet and BEVFusion.</li>
    </ul>
  </li>
  <li><strong>Efficiency Focus</strong>: Balancing high accuracy with minimal computation.</li>
</ul>

<h4 id="efficientvit-sam-accelerating-segment-anything-">EfficientViT-SAM: Accelerating ‚ÄúSegment Anything‚Äù ‚ö°</h4>
<p><img src="/assets/images/tinyml-2024/01/2.png" alt="alt_text" title="image_tooltip" />
EfficientViT-SAM enhances SAM-ViT-H by integrating EfficientViT, boosting speed without losing accuracy.</p>

<ul>
  <li><strong>What is ‚ÄúSegment Anything‚Äù?</strong> Models that can segment any object in an image with a prompt (e.g., point, box).</li>
  <li><strong>Efficiency Gains</strong>:
    <ul>
      <li><strong>48.9x Throughput</strong>: Processes 49x more images/sec on A100 GPU.</li>
      <li><strong>Zero-Shot Segmentation</strong>: Matches SAM-ViT-H‚Äôs performance without training.</li>
    </ul>
  </li>
  <li><strong>Advantages</strong>:
    <ul>
      <li><strong>Reduced Computation</strong>: Enables real-time use on limited hardware.</li>
      <li><strong>Preserved Accuracy</strong>: Same performance as SAM-ViT-H.</li>
    </ul>
  </li>
</ul>

<h3 id="3-bevfusion-for-3d-perception-">3. BEVFusion for 3D Perception üåê</h3>
<p><img src="/assets/images/tinyml-2024/01/3.png" alt="alt_text" title="image_tooltip" />
BEVFusion enables efficient 3D perception in self-driving:</p>
<ul>
  <li><strong>Sensor Input</strong>: Combines data from six cameras and LiDAR.</li>
  <li><strong>Tasks</strong>:
    <ul>
      <li><strong>3D Object Detection</strong>: Identifies objects in 3D, like cars and pedestrians.</li>
      <li><strong>BEV Map Segmentation</strong>: Creates a top-down map, segmenting areas like lanes and barriers.</li>
    </ul>
  </li>
  <li><strong>Efficiency</strong>: Runs on NVIDIA Jetson Orin, a mobile GPU, for real-time use.</li>
</ul>

<h3 id="4-natural-language-processing--large-language-models-">4. Natural Language Processing &amp; Large Language Models üìù</h3>
<ul>
  <li><strong>LLM Capabilities</strong>: Tasks include translation, code generation, few-shot learning.</li>
  <li><strong>Challenges</strong>: Bigger models demand more GPU memory, limiting edge deployment.</li>
  <li><strong>Efficient Techniques</strong>: Token pruning (SpAtten) and quantization (SmoothQuant, AWQ).</li>
  <li><strong>Edge Deployment</strong>: Privacy and offline needs drive edge deployment efforts.</li>
</ul>

<h3 id="5-multimodal-learning-and-vision-language-models-">5. Multimodal Learning and Vision-Language Models üé•üìù</h3>
<ul>
  <li><strong>Vision-Language Models</strong>: Covers models like LLaVA for image-text tasks (e.g., captioning).</li>
  <li><strong>Efficiency</strong>: Uses quantization methods (AWQ) for computational efficiency.</li>
  <li><strong>Data and Training</strong>: High-quality data and robust training essential for multimodal learning.</li>
</ul>

<h3 id="6-hardware-trends-and-the-role-of-software-">6. Hardware Trends and the Role of Software üíæ</h3>
<ul>
  <li><strong>GPU Trends</strong>: Higher performance and memory bandwidth with increased power usage.</li>
  <li><strong>Mobile &amp; Edge Hardware</strong>: Specialized AI units (e.g., Qualcomm Hexagon) but limited memory.</li>
  <li><strong>Microcontrollers</strong>: TinyML needs extreme efficiency due to tight memory constraints.</li>
  <li><strong>Software Optimization</strong>: Critical for maximizing hardware potential in complex models.</li>
</ul>

<h4 id="debriefing-ai-hardware-cloud-vs-edge-Ô∏è">Debriefing AI Hardware: Cloud vs. Edge üñ•Ô∏èüì±</h4>

<p>Cloud GPUs and edge devices highlight different strengths:</p>
<ul>
  <li><strong>Cloud GPUs</strong>: Performance gains from P100 to B100 (2024) offer 100x dense FP16 performance; memory bandwidth doubled.</li>
  <li><strong>Edge Hardware</strong>: Mobile AI units like Qualcomm‚Äôs Hexagon DSP allow efficient on-device AI, while Jetson targets high-performance applications.</li>
  <li><strong>Memory Gap</strong>: Cloud GPUs support GBs of memory, MCUs only a few KBs, necessitating model compression for edge use.</li>
</ul>

<h3 id="7-system-algorithm-co-design-">7. System-Algorithm Co-Design ü§ù</h3>
<ul>
  <li><strong>System-Algorithm Interaction</strong>: Understanding hardware-algorithm interplay is crucial.</li>
  <li><strong>Full-Stack Optimization</strong>: Combining EE, CS, and AI knowledge to drive innovation.</li>
</ul>

<h3 id="8-course-logistics-">8. Course Logistics üìÖ</h3>
<ul>
  <li><strong>Prerequisites</strong>: Computer architecture (6.191) and machine learning (6.390).</li>
  <li><strong>Hands-On Labs</strong>: Includes pruning, quantization, neural architecture search, LLM compression, and deployment.</li>
  <li><strong>Grading</strong>: Labs (75%), final project (25%), 4% participation bonus.</li>
</ul>

<hr />

<h2 id="-key-takeaways">üìù Key Takeaways</h2>
<ul>
  <li><strong>Efficiency as a Priority</strong>: Rising model complexity demands efficient solutions.</li>
  <li><strong>Model Compression</strong>: Techniques like pruning and quantization are essential.</li>
  <li><strong>System-Algorithm Co-Design</strong>: Holistic approach for high performance and efficiency.</li>
  <li><strong>Comprehensive Learning</strong>: Prepares students to tackle modern AI challenges.</li>
</ul>

<p>This lecture lays the foundation for exploring efficient deep learning throughout the semester.</p>

<h2 id="quiz-">Quiz üìù</h2>

<h3 id="instructions">Instructions</h3>
<p>Answer the following questions in <strong>2-3 sentences</strong> each.</p>

<hr />

<h3 id="questions-">Questions üîç</h3>

<ol>
  <li><strong>Why is efficient deep learning computing increasingly necessary?</strong></li>
  <li><strong>What event in 2012 marked a significant shift in the field of deep learning, and what was its primary contribution?</strong></li>
  <li><strong>What are two advantages of on-device training for AI systems?</strong></li>
  <li><strong>Describe one technique used to improve the efficiency of promptable image segmentation models.</strong></li>
  <li><strong>Why are diffusion models computationally expensive, especially for generating high-resolution images or videos?</strong></li>
  <li><strong>What is the primary challenge in aligning representations from different input modalities in multimodal learning?</strong></li>
  <li><strong>How does the concept of ‚Äúchain of thought‚Äù enhance the problem-solving capabilities of large language models?</strong></li>
  <li><strong>Explain the rationale behind using pruning techniques to accelerate language models.</strong></li>
  <li><strong>What is the main advantage of using quantization techniques like SmoothQuant and AWQ for deploying large language models on edge devices?</strong></li>
  <li><strong>Explain the concept of visual in-context learning in the context of vision-language models.</strong></li>
</ol>

<hr />

<h3 id="answer-key-Ô∏è">Answer Key üóùÔ∏è</h3>

<ol>
  <li>
    <p><strong>Efficient Deep Learning Computing Necessity</strong><br />
Efficient deep learning computing is crucial because the computational demands of AI models are increasing rapidly, outpacing hardware growth. This disparity creates challenges in cost, energy consumption, and deploying models on resource-constrained devices. ‚öôÔ∏è</p>
  </li>
  <li>
    <p><strong>2012 Breakthrough in Deep Learning</strong><br />
The advent of <em>AlexNet</em> in 2012 revolutionized deep learning by showcasing the effectiveness of CNNs for image classification on ImageNet. AlexNet‚Äôs deep architecture and GPU usage sparked significant advancements in computer vision and deep learning. üìà</p>
  </li>
  <li>
    <p><strong>On-Device Training Advantages</strong><br />
On-device training enhances <em>privacy</em> by processing data locally, removing the need to send sensitive data to the cloud. It also reduces <em>costs</em> by minimizing data transfer and cloud computing expenses, making AI applications more accessible. üîíüí∏</p>
  </li>
  <li>
    <p><strong>Efficient Promptable Image Segmentation</strong><br />
EfficientViT is an optimized Vision Transformer (ViT) architecture used for promptable image segmentation. It speeds up segmentation models like SAM by minimizing redundancy and streamlining the processing pipeline. üöÄ</p>
  </li>
  <li>
    <p><strong>Computational Cost of Diffusion Models</strong><br />
Diffusion models are computationally intensive due to their iterative denoising process. Generating high-resolution images or videos requires numerous steps, each needing substantial computation. üíª</p>
  </li>
  <li>
    <p><strong>Challenge in Multimodal Learning</strong><br />
The primary challenge in multimodal learning is aligning different input modalities (images, text, audio) into a shared representation space. This alignment is essential for the model to understand relationships between modalities and make accurate predictions. üé®üìù</p>
  </li>
  <li>
    <p><strong>‚ÄúChain of Thought‚Äù in Large Language Models</strong><br />
‚ÄúChain of thought‚Äù prompting guides LLMs to break down problems into smaller reasoning steps, enabling more logical solutions. This approach improves the model‚Äôs ability to solve complex tasks. üîóüß†</p>
  </li>
  <li>
    <p><strong>Pruning for Language Model Acceleration</strong><br />
Pruning reduces model size by removing redundant connections in neural networks, or tokens in language models, to improve efficiency. This approach speeds up inference without significantly impacting performance. ‚úÇÔ∏è</p>
  </li>
  <li>
    <p><strong>Quantization for Edge Device Deployment</strong><br />
Techniques like SmoothQuant reduce numerical precision of model parameters, typically from 16-bit to lower bit-widths like 4-bit. This allows large models to fit within edge device memory while maintaining accuracy. üî¢üì±</p>
  </li>
  <li>
    <p><strong>Visual In-Context Learning in Vision-Language Models</strong><br />
Visual in-context learning enables models to understand tasks from a few labeled examples, without explicit instructions. For example, a model can learn to identify cities in images after being shown a few image-city pairs. üñºÔ∏èüîç</p>
  </li>
</ol>

<hr />

<h3 id="essay-questions-Ô∏è">Essay Questions üñãÔ∏è</h3>

<ol>
  <li>
    <p><strong>Trade-Offs Between Model Accuracy and Efficiency</strong><br />
Discuss the trade-offs in deploying deep learning models on edge devices, considering computational resources, power constraints, and application requirements.</p>
  </li>
  <li>
    <p><strong>Neural Architecture Search (NAS) in Efficient Model Development</strong><br />
Explain the role of NAS in designing efficient deep learning models. Compare different NAS approaches, their strengths, and limitations.</p>
  </li>
  <li>
    <p><strong>Challenges of Training Large Language Models on Resource-Constrained Devices</strong><br />
Describe techniques to reduce the computational and memory requirements of training LLMs on resource-constrained devices. Discuss associated challenges and opportunities.</p>
  </li>
  <li>
    <p><strong>Ethical Considerations in Deploying Efficient Deep Learning Models</strong><br />
Consider the ethics of deploying efficient AI models in areas like facial recognition and autonomous driving, including potential biases and privacy concerns.</p>
  </li>
  <li>
    <p><strong>Future Directions in Efficient Deep Learning Computing</strong><br />
Explore future directions in efficient deep learning, considering new hardware architectures, novel compression techniques, and AI sustainability.</p>
  </li>
</ol>

<hr />

<h3 id="glossary-">Glossary üìò</h3>

<ul>
  <li><strong>AlexNet</strong>: A groundbreaking CNN architecture that highlighted deep learning‚Äôs power for image classification.</li>
  <li><strong>Chain of Thought Prompting</strong>: Enhances LLM reasoning by guiding them to break down complex problems into steps.</li>
  <li><strong>Diffusion Model</strong>: Generative model creating images/videos via iterative denoising from random noise.</li>
  <li><strong>Efficient Deep Learning Computing</strong>: Field dedicated to reducing computational and memory demands of deep learning, enabling deployment on resource-limited devices.</li>
  <li><strong>GAN Compression</strong>: Reduces computational cost in GANs through pruning, knowledge distillation, etc.</li>
  <li><strong>ImageNet</strong>: Large dataset of labeled images for training/evaluating computer vision models.</li>
  <li><strong>Large Language Model (LLM)</strong>: Model trained on massive text data, generating human-like text and language tasks.</li>
  <li><strong>Multimodal Learning</strong>: Processes and understands information from multiple modalities (images, text, etc.).</li>
  <li><strong>Neural Architecture Search (NAS)</strong>: Automates neural network design, optimizing configurations.</li>
  <li><strong>On-Device Training</strong>: Training AI models directly on edge devices, benefiting privacy, cost, and customization.</li>
  <li><strong>Pruning</strong>: Compresses networks by removing connections/neurons deemed less important.</li>
  <li><strong>Quantization</strong>: Reduces precision of numerical values in model parameters, lowering model size.</li>
  <li><strong>SmoothQuant</strong>: Quantization technique that smooths activation distribution, improving quantization.</li>
  <li><strong>TinyML</strong>: Focuses on deploying deep learning models on ultra-low-power microcontrollers.</li>
  <li><strong>Vision Transformer (ViT)</strong>: Applies transformer architecture (for language) to vision tasks.</li>
  <li><strong>Visual In-Context Learning</strong>: Vision-language model capability to infer tasks from few visual examples.</li>
  <li><strong>Visual Language Model</strong>: Trained on image-text data, understanding and generating visual and textual info.</li>
</ul>

<hr />]]></content><author><name>Leon Liu</name></author><category term="Course" /><category term="TLDR" /><category term="tinyml" /><category term="llm" /><summary type="html"><![CDATA[TinyML 2024 Lecture 1 üå±]]></summary></entry><entry><title type="html">Introduction of Quantization</title><link href="http://localhost:4000/llm/2024/06/01/quantization-introduction.html" rel="alternate" type="text/html" title="Introduction of Quantization" /><published>2024-06-01T00:00:00-04:00</published><updated>2024-06-01T00:00:00-04:00</updated><id>http://localhost:4000/llm/2024/06/01/quantization-introduction</id><content type="html" xml:base="http://localhost:4000/llm/2024/06/01/quantization-introduction.html"><![CDATA[<h1 id="overview-of-quantization">Overview of Quantization</h1>

<!-- toc -->

<ul>
  <li><a href="#motivation">Motivation</a></li>
  <li><a href="#data-types">Data Types</a></li>
  <li><a href="#quantization-basics">Quantization Basics</a>
    <ul>
      <li><a href="#quantization-target">Quantization Target</a></li>
      <li><a href="#post-training-quantization-ptq">Post-Training Quantization (PTQ)</a></li>
      <li><a href="#quantization-aware-training-qat">Quantization-Aware Training (QAT)</a></li>
    </ul>
  </li>
  <li><a href="#quantization-introduction">Quantization Introduction</a>
    <ul>
      <li><a href="#k-means-based-weight-quantization">K-means-based Weight Quantization</a></li>
      <li><a href="#linear-quantization">Linear Quantization</a></li>
      <li><a href="#binary-and-ternary-quantization">Binary and Ternary Quantization</a></li>
      <li><a href="#automatic-mixed-precision-quantization">Automatic Mixed-precision Quantization</a></li>
    </ul>
  </li>
  <li><a href="#quantization-for-llms">Quantization for LLMs</a>
    <ul>
      <li><a href="#llmint8">LLM.int8()</a></li>
      <li><a href="#gptq">GPTQ</a></li>
      <li><a href="#smoothquant-w8a8">SmoothQuant (W8A8)</a></li>
      <li><a href="#awq">AWQ</a></li>
      <li><a href="#the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></li>
      <li><a href="#gguf">GGUF</a></li>
    </ul>
  </li>
  <li><a href="#toolkit-and-code-examples">Toolkit and Code Examples</a></li>
  <li><a href="#reference">Reference</a></li>
</ul>

<!-- tocstop -->

<h2 id="motivation">Motivation</h2>

<p>Why Do We Need Quantization? Today‚Äôs AI is too big. While a larger model offers more capabilities, it also demands more expensive hardware and greater hardware resources. Solutions: Model Distillation, Quantization, etc for efficient inference</p>

<p><img src="/assets/images/quantization-intro/image1.png" alt="alt_text" title="image_tooltip" /></p>

<p>What is quantization?</p>

<ul>
  <li>Quantization = mapping continuous infinite values to a smaller set of discrete finite values</li>
  <li>In the context of LLMs, it refers to the process of converting the weights of the model from higher-precision data types to lower-precision ones.</li>
</ul>

<h2 id="data-types">Data Types</h2>

<p>How is numeric data represented?</p>

<ul>
  <li>Integer
    <ul>
      <li>Unsigned integer</li>
      <li>Signed integer</li>
    </ul>
  </li>
  <li>Fixed-point number</li>
  <li>Floating-point number</li>
</ul>

<p><img src="/assets/images/quantization-intro/image2.png" alt="alt_text" title="image_tooltip" /></p>

<p>In the context of quantization, floating-point numbers and integers represent different ways to encode numerical values with varying precision. Here‚Äôs an explanation with examples:</p>

<ul>
  <li>Floating-Point Numbers:
    <ul>
      <li>Floating-point numbers are numbers that have both an integer part and a fractional part, separated by a decimal point. They are represented in computer systems using a sign bit, an exponent, and a fraction.</li>
      <li>Example: 3.14, -0.001, 123.456</li>
      <li>In binary floating-point representation (common in computer systems), the number 3.14 might be represented as:
        <ul>
          <li>Sign bit: 0 (positive)</li>
          <li>Exponent: 10000000 (binary for 128, representing an exponent of 4)</li>
          <li>Fraction: 1001001000011110101110000101000111101011100001010001111010111000 (binary representation of the fractional part)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Integers:
    <ul>
      <li>Integers are whole numbers without any fractional part. They can be positive, negative, or zero.</li>
      <li>Example: -5, 0, 42</li>
      <li>In binary integer representation, each digit in the binary number represents a power of 2. For example, the decimal number 42 might be represented as 101010 in binary.</li>
      <li>Integers require less memory and computational resources compared to floating-point numbers because they do not store fractional parts. They are often used in applications where precise fractional values are not required.</li>
    </ul>
  </li>
</ul>

<p>In quantization, floating-point numbers are typically converted to fixed-point representations, which can include integers of various precisions (e.g., 8-bit, 16-bit) to reduce memory usage and computational complexity. For example, instead of storing weights and activations as floating-point numbers, they might be quantized to 8-bit integers, which requires less memory and can be processed more efficiently on hardware platforms with limited resources.</p>

<p><img src="/assets/images/quantization-intro/image3.png" alt="alt_text" title="image_tooltip" /></p>

<p><img src="/assets/images/quantization-intro/image4.png" alt="alt_text" title="image_tooltip" /></p>

<h2 id="quantization-basics">Quantization Basics</h2>

<h3 id="quantization-target">Quantization Target</h3>

<p>When discussing quantization, there are typically several targets for quantization: model weights, activation functions, key-value (KV) caches, and gradients.</p>

<ul>
  <li>Weight</li>
  <li>Activation</li>
  <li>KV cache</li>
  <li>Gradients</li>
</ul>

<p>Model quantization techniques are pivotal in compressing neural network models to make them more memory and computationally efficient, particularly for deployment on resource-constrained devices. One common approach is to quantize model weights, which involves representing the weights with reduced precision compared to floating-point numbers. This typically entails converting weights from 32-bit floating-point to 8-bit integers or even lower bit-width representations. While this reduction in precision may result in a slight loss of model accuracy, it significantly reduces the memory footprint and speeds up computations.</p>

<p>In addition to quantizing model weights, another key aspect is quantizing input activations. This involves converting the input data passed through the neural network into lower precision formats before performing computations. By quantizing input activations, the computational cost can be further reduced, as operations on lower precision data require fewer computational resources. However, careful calibration and scaling are necessary to minimize the impact on model accuracy.</p>

<p>Key-value (KV) cache quantization is another technique employed to optimize memory usage during inference. KV caches store intermediate results of computations, which are reused multiple times within the inference process. By quantizing these cached values, memory usage can be significantly reduced without compromising inference accuracy. However, quantizing KV caches requires careful consideration of the trade-off between memory savings and the potential impact on model performance.</p>

<p>Furthermore, quantizing gradients during training can accelerate the optimization process, particularly in scenarios where the computational resources are limited. By quantizing gradients, the memory requirements during backpropagation can be reduced, allowing for larger batch sizes or deeper networks to be trained within the available resources. However, quantizing gradients introduces additional challenges such as gradient clipping and loss of gradient information, which must be addressed to ensure convergence and stability during training. Overall, model quantization techniques play a crucial role in enabling the deployment of deep learning models on a wide range of devices with varying computational constraints.</p>

<h3 id="post-training-quantization-ptq">Post-Training Quantization (PTQ)</h3>

<p>Quantizes a floating-point neural network model, including: channel quantization, group quantization, and range clipping.</p>

<p>Post-Training Quantization (PTQ): PTQ applies quantization after the model has been trained using full precision (typically floating-point arithmetic).</p>

<p>In PTQ, the weights and activations of the trained model are quantized to lower precision representations (such as 8-bit integers) without retraining the model from scratch. This quantization process helps reduce the memory footprint and computational complexity of the model, making it more suitable for deployment on resource-constrained devices like mobile phones or embedded systems.</p>

<p>PTQ typically involves techniques like weight clustering, where weights are grouped into clusters and represented by a single shared value, and activation quantization, where input data are quantized before passing through activation functions. PTQ is relatively straightforward to implement and can yield significant reductions in model size and inference latency with minimal impact on accuracy, especially when combined with techniques like fine-tuning or calibration to mitigate any performance degradation caused by quantization.</p>

<ul>
  <li>Post-training dynamic quantization (PTDQ) directly transforms each layer by quantizing it using a quantization formula without the need for a calibration dataset.</li>
  <li>Post-training calibration quantization (PTCQ) requires a representative dataset as input and adjusts the quantization weights based on the input and output of each layer in the model. GPTQ utilizes this method.</li>
</ul>

<h3 id="quantization-aware-training-qat">Quantization-Aware Training (QAT)</h3>

<p>Emulates inference-time quantization during the training/fine-tuning and recover the accuracy.</p>

<p>Quantization-Aware Training (QAT): Unlike PTQ, QAT integrates the weight conversion process during the training stage. This technique involves training a neural network while considering the effects of quantization during both forward and backward passes. By incorporating quantization into the training process, the model learns to adapt to the reduced precision of weights and activations. This can lead to better performance when the quantized model is deployed for inference, as it has been trained to accommodate the quantization-induced errors. This often results in superior model performance, but it‚Äôs more computationally demanding.</p>

<p>Quantization during finetuning sometimes known as Quantization-Aware Fine-tuning (QAF). After training a neural network using QAT or a similar technique, the quantized model may still require fine-tuning to further optimize its performance. QAF involves fine-tuning the quantized model using a smaller learning rate to refine its parameters and improve its accuracy or other performance metrics. This fine-tuning process specifically targets the nuances introduced by quantization, helping to mitigate any degradation in performance caused by the quantization process. QLoRA employs this approach.</p>

<h2 id="quantization-introduction">Quantization Introduction</h2>

<h3 id="k-means-based-weight-quantization">K-means-based Weight Quantization</h3>

<p><a href="https://arxiv.org/pdf/1510.00149.pdf">Paper: Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a></p>

<p>[Han, S., Mao, H., &amp; Dally, W. J. (2015). Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. <em>ArXiv</em>. /abs/1510.00149]</p>

<p>K-means-based weight quantization is a technique used in machine learning and particularly in deep learning to reduce the precision of model weights, thereby reducing memory requirements and potentially speeding up computations.</p>

<p>Here‚Äôs a step-by-step explanation of how it works:</p>

<ul>
  <li>Initialization: Initially, the weights of the neural network are typically represented as high-precision floating-point numbers, often 32-bit or 64-bit. These high precision weights require significant memory and computational resources.</li>
  <li>K-means Clustering: The first step in K-means-based weight quantization is to apply the K-means clustering algorithm to the weights. In this step, the weights are treated as vectors in a high-dimensional space, and the K-means algorithm is used to partition these vectors into K clusters.</li>
  <li>Centroid Calculation: Once the clusters are formed, the centroid of each cluster is computed. These centroids represent the quantized values that will replace the original weights.</li>
  <li>Quantization: After calculating the centroids, each weight in the neural network is replaced with the centroid value of the cluster to which it belongs. In other words, each weight is quantized to one of the K centroids.</li>
  <li>Encoding: To efficiently store the quantized weights, additional steps may be taken to encode them using a smaller number of bits. For example, instead of using 32 bits to represent each weight, they might be encoded using only 8 bits.</li>
  <li>Decoding: During inference, the quantized weights are decoded back to their original representation using the centroids assigned during quantization.</li>
</ul>

<p>Benefits of K-means-based weight quantization include:</p>

<ul>
  <li>Memory Reduction: By quantizing weights to a smaller number of centroids, the memory required to store the weights is significantly reduced.</li>
  <li>Speedup in Inference: The reduced memory footprint can lead to faster inference times due to reduced memory bandwidth requirements and potentially more efficient cache usage.</li>
  <li>Minimal Loss in Accuracy: With careful selection of the number of centroids and optimization of the quantization process, it‚Äôs possible to minimize the loss in model accuracy even after quantizing the weights.</li>
</ul>

<p>However, it‚Äôs important to note that quantizing weights can sometimes lead to a loss of model accuracy, especially if not done carefully. The choice of the number of centroids (K) and the quantization method are critical factors in minimizing this loss. Additionally, the computational cost of applying K-means clustering and the overhead of encoding and decoding weights should be considered when deciding whether to use this technique.</p>

<p><img src="/assets/images/quantization-intro/image5.png" alt="alt_text" title="image_tooltip" /></p>

<p>The weights are decompressed using a lookup table (i.e., codebook) during runtime inference.</p>

<ul>
  <li>K-Means-based Weight Quantization only saves storage cost of a neural network model.</li>
  <li>All the computation and memory access are still floating-point.</li>
</ul>

<h3 id="linear-quantization">Linear Quantization</h3>

<p><a href="https://arxiv.org/pdf/1712.05877.pdf">Paper: Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></p>

<p>[Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., &amp; Kalenichenko, D. (2017). Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. <em>ArXiv</em>. /abs/1712.05877]</p>

<p>An affine mapping of integers to real numbers.</p>

<p><img src="/assets/images/quantization-intro/image6.png" alt="alt_text" title="image_tooltip" /></p>

<p><img src="/assets/images/quantization-intro/image7.png" alt="alt_text" title="image_tooltip" /></p>

<p>Linear quantization is a technique used to reduce the precision of numerical values, typically in the context of deep learning model parameters or activations. The process involves mapping high-precision floating-point numbers to a smaller set of discrete values while attempting to preserve the overall behavior and performance of the model.</p>

<p>Here‚Äôs how linear quantization generally works:</p>

<ul>
  <li>Initialization: Similar to other quantization methods, linear quantization starts with high-precision floating-point numbers representing model parameters or activations. These values are typically stored using 32-bit floating-point representation, which consumes considerable memory and computational resources.</li>
  <li>Range Determination: In linear quantization, the first step is to determine the range of values present in the data to be quantized. This involves finding the minimum and maximum values across all parameters or activations.</li>
  <li>Quantization Levels: After determining the range, the next step is to divide this range into a predefined number of equally spaced intervals, known as quantization levels or bins. These bins represent the discrete values that will replace the original floating-point numbers.</li>
  <li>Mapping: Each floating-point value is then mapped to the nearest quantization level based on its proximity. This mapping can be performed using simple linear interpolation or rounding techniques.</li>
  <li>Encoding: Once the mapping is done, the quantized values are typically encoded using a smaller number of bits. For example, instead of using 32 bits for floating-point representation, the quantized values might be encoded using 8 bits, 4 bits, or even fewer bits.</li>
  <li>Decoding: During inference, the quantized values are decoded back to their original floating-point representation before being used in computations.</li>
</ul>

<p>Linear quantization offers several advantages:</p>

<ul>
  <li>Memory Reduction: By reducing the precision of numerical values and encoding them with fewer bits, linear quantization significantly reduces the memory requirements for storing model parameters and activations.</li>
  <li>Speedup in Inference: The reduced memory footprint can lead to faster inference times due to reduced memory bandwidth requirements and potentially more efficient cache usage.</li>
  <li>Straightforward Implementation: Linear quantization is relatively straightforward to implement and understand compared to more complex quantization techniques.</li>
</ul>

<p>However, linear quantization may lead to a loss of model accuracy, especially if not applied carefully. The choice of the number of quantization levels and the range determination method are critical factors in minimizing this loss. Additionally, careful consideration should be given to the impact of quantization on the overall behavior and performance of the model, especially in tasks where precision is crucial.</p>

<h3 id="binary-and-ternary-quantization">Binary and Ternary Quantization</h3>

<p>Binary and ternary quantization are specialized techniques used to further compress neural network models by reducing the precision of weights or activations to just two or three discrete levels, respectively.</p>

<p>Binary Quantization: In binary quantization, each weight or activation is represented using just two discrete levels: typically -1 and +1 or 0 and 1. This extreme reduction in precision drastically reduces the memory footprint and computational complexity of the model, making it highly efficient for deployment on low-power devices. Binary quantization is achieved by thresholding the floating-point values of weights or activations to their nearest binary representation. Although binary quantization can lead to significant model compression, it often comes with a noticeable degradation in accuracy compared to higher precision models.</p>

<p><img src="/assets/images/quantization-intro/image8.png" alt="alt_text" title="image_tooltip" /></p>

<p>Ternary Quantization: Ternary quantization extends the idea of binary quantization by allowing weights or activations to take on three discrete levels, typically -1, 0, and +1. By introducing a neutral zero level, ternary quantization offers a compromise between model compression and accuracy preservation compared to binary quantization. Ternary quantization is achieved through techniques such as thresholding or clustering, where weights or activations are assigned to the nearest of the three discrete levels. While ternary quantization is not as aggressive in compression as binary quantization, it often achieves a better balance between model size reduction and accuracy retention.</p>

<p><img src="/assets/images/quantization-intro/image9.png" alt="alt_text" title="image_tooltip" /></p>

<p>Both binary and ternary quantization techniques are particularly valuable for deploying neural network models on devices with stringent memory and computational constraints, such as mobile phones, IoT devices, or edge computing devices. However, achieving optimal performance with these techniques requires careful calibration, fine-tuning, and possibly architectural adjustments to mitigate any loss in accuracy introduced by the drastic reduction in precision.</p>

<h3 id="automatic-mixed-precision-quantization">Automatic Mixed-precision Quantization</h3>

<p>Automatic mixed-precision quantization optimizes neural network models by dynamically adjusting precision levels for parameters or layers. It balances computational efficiency and accuracy by using higher precision where needed and lower precision elsewhere. Techniques include dynamic scaling, layer-wise adjustment, and adaptive schemes. This approach is vital for deploying models on diverse hardware efficiently.</p>

<p><img src="/assets/images/quantization-intro/image10.png" alt="alt_text" title="image_tooltip" /></p>

<h2 id="quantization-for-llms">Quantization for LLMs</h2>

<h3 id="llmint8">LLM.int8()</h3>

<p><a href="https://arxiv.org/pdf/2208.07339.pdf">Paper: LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></p>

<p><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">Huggingface implementation</a></p>

<p>This paper presents LLM.int8(), a method for performing 8-bit matrix multiplication for transformer language models of up to 175 billion parameters without any performance degradation compared to the full 16-bit precision models. The key contributions are:</p>

<ul>
  <li>Vector-wise quantization that uses separate quantization normalization constants for each inner product in the matrix multiplication to improve quantization precision up to 2.7B parameters.</li>
  <li>Analysis of emergent large magnitude ‚Äúoutlier‚Äù features in transformer layers that start appearing around 6.7B parameters and disrupt quantization precision.</li>
  <li>Mixed-precision decomposition that performs 16-bit matrix multiplication for the outlier feature dimensions (0.1% of values) and 8-bit for the rest, allowing full precision on large models.</li>
  <li>Showing LLM.int8() enables inference on models like OPT-175B and BLOOM on a single consumer GPU server while reducing memory footprint by half compared to 16-bit precision.</li>
</ul>

<p>The paper provides insights into the characteristics of outlier features across transformer scales and their impact on attention and predictive performance. LLM.int8() makes very large language models more accessible by eliminating the performance degradation from naive 8-bit quantization methods.</p>

<p><img src="/assets/images/quantization-intro/image11.png" alt="alt_text" title="image_tooltip" /></p>

<p>Currently, the implementation of LLM.int8() primarily resides in the bitsandbytes library, which has been integrated into the transformers library and natively supports it. Meanwhile, another quantization scheme, QLoRA, proposed by the author of LLM.int8(), is also implemented based on bitsandbytes.</p>

<h3 id="gptq">GPTQ</h3>

<p><a href="https://arxiv.org/pdf/2210.17323.pdf">Paper: GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PR E-TRAINED TRANSFORMERS</a></p>

<p>This paper introduces GPTQ, a new method for quantizing large language models like GPT and OPT to low bitwidths (e.g. 3 or 4 bits per weight) with minimal accuracy degradation. The key contributions are:</p>

<p>GPTQ is a one-shot post-training quantization method that can quantize models with hundreds of billions of parameters in just a few hours on a single GPU, while preserving accuracy. Previous methods either sacrificed accuracy at high compression rates or did not scale to such large models. It is based on approximate second-order information and quantizes weights in a specific order while updating the remaining weights to compensate for error. Several optimizations are proposed to make this computationally tractable for giant models.</p>

<p>GPTQ can quantize models like OPT-175B and BLOOM-176B down to 3-4 bits per weight with negligible increase in perplexity on language modeling tasks, outperforming simple rounding methods.</p>

<p><img src="/assets/images/quantization-intro/image12.png" alt="alt_text" title="image_tooltip" /></p>

<p>Here‚Äôs a summary of the three key steps in the GPTQ algorithm:</p>

<p>Step 1: Arbitrary Order Insight</p>

<ul>
  <li>The authors find that quantizing weights in an arbitrary fixed order (instead of the greedy optimal order used in previous methods) works almost as well, especially for large over-parameterized layers.</li>
  <li>This allows them to quantize all rows of a weight matrix in the same order, reducing the complexity from O(d_row * d_col^3) to O(max{d_row * d_col^2, d_col^3}).</li>
</ul>

<p>Step 2: Lazy Batch-Updates</p>

<ul>
  <li>Directly applying the updates has a low compute-to-memory-access ratio, leading to underutilization of GPU resources.</li>
  <li>To mitigate this, updates are ‚Äúlazily batched‚Äù by quantizing B=128 columns at a time and only updating the corresponding BxB block of the inverse Hessian matrix.</li>
  <li>Global updates to the full weight matrix are performed once the block is quantized, improving GPU utilization.</li>
</ul>

<p>Step 3: Cholesky Reformulation</p>

<ul>
  <li>Repeated updates to the inverse Hessian can lead to numerical issues, especially for giant models.</li>
  <li>Instead of directly updating the inverse, the Cholesky decomposition of the inverse Hessian is precomputed, which is more numerically stable.</li>
  <li>The required information is extracted from the Cholesky factors in a robust manner during quantization.</li>
</ul>

<p>Readings:</p>

<p><a href="https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34">https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34</a></p>

<h3 id="smoothquant-w8a8">SmoothQuant (W8A8)</h3>

<p><a href="https://arxiv.org/pdf/2211.10438.pdf">Paper: SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a></p>

<p>SmoothQuant is a post-training quantization (PTQ) method that ensures both accuracy and efficient inference, capable of achieving 8-bit weight and 8-bit activation (W8A8) quantization. Since weights are easy to quantize while activations pose a greater challenge, The key idea is to ‚Äúsmooth‚Äù the activation outliers by migrating the quantization difficulty from activations to weights through a mathematically equivalent transformation. This makes both weights and activations easy to quantize with low-precision integers.</p>

<p><img src="/assets/images/quantization-intro/image13.png" alt="alt_text" title="image_tooltip" /></p>

<h3 id="awq">AWQ</h3>

<p><a href="https://arxiv.org/pdf/2306.00978.pdf">Paper: AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></p>

<p>LLM decoding is highly memory-bounded; W8A8 is not enough</p>

<ul>
  <li>W8A8 quantization is good for batch serving (e.g., batch size 128)</li>
  <li>But single-query LLM inference (e.g., local) is still highly memory-bounded</li>
  <li>We need low-bit weight-only quantization (e.g., W4A16) for this setting</li>
</ul>

<p><img src="/assets/images/quantization-intro/image14.png" alt="alt_text" title="image_tooltip" /></p>

<p>We find that weights are not equally important, keeping only 1% of salient weight channels in FP16 can greatly improve perplexity. How to find the weights? We should look for activation distribution, but not weight.</p>

<p>Multiplying the salient channels with S&gt;1 reduces its quantization error like smoothQuant. Also no mixed precision weights and can be easier to train.</p>

<p><img src="/assets/images/quantization-intro/image15.png" alt="alt_text" title="image_tooltip" /></p>

<h3 id="the-era-of-1-bit-llms-all-large-language-models-are-in-158-bits">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</h3>

<p><a href="https://arxiv.org/pdf/2402.17764.pdf">Paper: The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits.</a></p>

<p>[Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., &amp; Wei, F. (2024). The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. ArXiv. /abs/2402.17764]</p>

<ul>
  <li>Based on another paper: BitNet: Scaling 1-bit Transformers for Large Language Models</li>
  <li>Matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end task performance</li>
  <li>While being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption.</li>
</ul>

<h3 id="gguf">GGUF</h3>
<p>GGML, a C library created by Georgi Gerganov, focuses on machine learning (ML). Alongside fundamental ML elements like tensors, it introduces a distinct binary format, GGUF, for distributing LLMs (Large Language Models). GGUF aims for extensibility, ensuring new features won‚Äôt disrupt compatibility with existing models. Notably, it consolidates metadata into one file, addressing past challenges and ensuring future adaptability. GGML models encompass those using GGUF or prior formats. GGML collaborates with llama.cpp, another of Gerganov‚Äôs creations, for efficient Llama model inference. Originally designed for CPU execution, llama.cpp now enables GPU offloading for certain layers, notably enhancing inference speed for larger LLMs that exceed GPU memory capacity.</p>

<p>Readings:</p>

<p><a href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html">https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html</a></p>

<h2 id="toolkit-and-code-examples">Toolkit and Code Examples</h2>

<p><img src="/assets/images/quantization-intro/image16.png" alt="alt_text" title="image_tooltip" /></p>

<p>Some useful quantization tools: TGI, bitsandbytes, vLLM, tensorRT-LLM, AutoGPTQ, AutoAWQ</p>

<p>Readings:</p>

<p><a href="https://www.maartengrootendorst.com/blog/quantization/">https://www.maartengrootendorst.com/blog/quantization/</a></p>

<h2 id="reference">Reference</h2>

<p><a href="https://hanlab.mit.edu/courses/2023-fall-65940">https://hanlab.mit.edu/courses/2023-fall-65940</a>: lecture 5,6 and 13</p>

<p><a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">https://lilianweng.github.io/posts/2023-01-10-inference-optimization/</a></p>

<p><a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/">https://pytorch.org/blog/introduction-to-quantization-on-pytorch/</a></p>

<p><a href="https://pytorch.org/docs/stable/quantization.html#best-practices">https://pytorch.org/docs/stable/quantization.html#best-practices</a></p>]]></content><author><name>Leon Liu</name></author><category term="LLM" /><category term="quantization" /><category term="llm" /><summary type="html"><![CDATA[Overview of Quantization]]></summary></entry></feed>